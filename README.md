# Remaining Useful Life (RUL) Prediction using Transformer and BiLSTM Models

This project explores predictive maintenance by estimating the Remaining Useful Life (RUL) of aircraft engines using deep learning models. Two architectures were evaluated on the complex **NASA CMAPSS FD004** dataset: a **Transformer** and a **Bidirectional LSTM (BiLSTM)**. The Transformer model achieved the best performance.

---

## ğŸ“Œ Problem Statement

* **Dataset:** NASA CMAPSS FD004
* **Engines:** 248 (train), 249 (test)
* **Fault Modes:** 2 (HPC degradation and Fan degradation)
* **Operational Conditions:** 6 unique profiles
* **Objective:** Predict Remaining Useful Life (RUL) for engines in unseen test data

---

## âš™ï¸ Data Preprocessing

* **Normalization:** StandardScaler applied to all 24 features
* **Features:**

  * 3 operational settings
  * 21 sensor readings
* **Windowing Strategy:**

  * Sliding window of 30 time steps (stride = 1)
  * Input shape: `(batch_size, 30, 24)`
* **RUL Label Clipping:**

  * Maximum capped at 125 cycles to avoid bias toward high RULs

---

## ğŸ§  Model Architectures

### ğŸ”¹ Transformer

* Input Shape: `(batch_size, 30, 24)`
* Layers:

  * 3 Transformer Encoder layers
  * 4 Attention Heads per layer
* Hyperparameters:

  * `d_model = 64`, `dim_feedforward = 128`
  * Dropout: 0.1

### ğŸ”¹ BiLSTM

* Input Shape: `(batch_size, 30, 24)`
* Layers:

  * 2 BiLSTM layers with 64 hidden units
* Dropout: 0.2

---

## ğŸ‹ï¸ Training Configuration

* **Train/Validation Split:** 80/20 random split on windowed data
* **Loss Function:** Mean Squared Error (MSE)
* **Optimizer:** Adam (`learning_rate = 0.001`)
* **Batch Size:** 128
* **Epochs:** 30
* **Early Stopping:** Patience of 5 epochs

---

## ğŸ“Š Evaluation Metrics (Test Set)

**Best Model: Transformer**

* âœ… **MAE:** 22.49 cycles
* âœ… **RMSE:** 29.70 cycles
* âœ… **RÂ² Score:** 0.7032

### ğŸ“ˆ Plot: Predicted vs True RUL

---

## ğŸ“ Repository Structure

```
.
â”œâ”€â”€ data/                # Preprocessed or raw CMAPSS data
â”œâ”€â”€ models/              # Saved Transformer/BiLSTM models
â”œâ”€â”€ notebooks/           # Model training and evaluation notebooks
â””â”€â”€ README.md            # Project overview (you are here)
```

---

## ğŸ§  Key Learnings & Reflections

* The **Transformer outperformed BiLSTM** on the complex FD004 dataset due to better long-range dependency modeling.
* BiLSTM suffered from **overfitting** and struggled to generalize.
* Saturation in RUL predictions above 120 cycles was observed due to **data imbalance**.
* Visuals show the model is particularly reliable in **low RUL regions**, which is critical for maintenance planning.

---

## ğŸ”® Future Improvements

* Add a **CNN + LSTM hybrid** model
* Explore **attention visualization** for interpretability
* Try **uncertainty estimation** (e.g., MC Dropout) for confidence bounds on RUL

---




---

## ğŸ“¬ Contact

If you have any questions or feedback, feel free to reach out at \[[your.email@example.com](mailto:your.email@example.com)] or open an issue in the repo.
